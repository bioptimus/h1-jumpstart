{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy H-optimus-1 Model Package from AWS Marketplace \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H-optimus-1 is a foundation model for histology, developed by Bioptimus. \n",
    "\n",
    "The model is a 1.1B parameter vision transformer trained on a proprietary collection of more than 1 million H&E stained whole slide histology images. For more information, please refer to this [page](https://www.bioptimus.com/h-optimus-1).\n",
    "\n",
    "H-optimus-1 can extract powerful features from histology images for various downstream applications, such as mutation prediction, survival analysis, or tissue classification.\n",
    "\n",
    "This sample notebook shows you how to deploy H-optimus-1 using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to [H-optimus-1](https://aws.amazon.com/marketplace/pp/prodview-cuad7l27fobx4). If so, skip step: [Subscribe to the model package](#1.-Subscribe-to-the-model-package).\n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Create input payload](#B.-Create-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "   4. [Visualize output](#D.-Visualize-output)\n",
    "   5. [Delete the endpoint](#E.-Delete-the-endpoint)\n",
    "3. [Perform batch inference](#3.-Perform-batch-inference) \n",
    "4. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    2. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the model package listing page [H-optimus-1](https://aws.amazon.com/marketplace/pp/prodview-cuad7l27fobx4).\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_arn = \"arn:aws:sagemaker:eu-north-1:136758871317:model-package/h-optimus-1-7f16e68f69cf3b7bb608d126ac6b9a99\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code was executed with python 3.13.7\n",
    "%pip install sagemaker==\"2.254.1\" \n",
    "%pip install pillow==\"11.1.0\"\n",
    "%pip install boto3==\"1.42.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "from PIL import Image as ImageEdit\n",
    "\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sage.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "runtime = boto3.client(\"runtime.sagemaker\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"h-optimus-1\"\n",
    "content_type = \"image/*\"\n",
    "real_time_inference_instance_type = \"ml.g5.xlarge\"\n",
    "batch_transform_inference_instance_type = \"ml.g5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deployable model from the model package.\n",
    "model = ModelPackage(\n",
    "    role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model.\n",
    "predictor = model.deploy(\n",
    "    1,\n",
    "    real_time_inference_instance_type,\n",
    "    endpoint_name=model_name,\n",
    "    inference_ami_version=\"al2-ami-sagemaker-inference-gpu-3-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create input payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = \"data/input/real-time/example_input.png\"\n",
    "img = ImageEdit.open(image_file)\n",
    "# Save the image to a byte stream in PNG format\n",
    "buffer = BytesIO()\n",
    "img.save(buffer, format=\"PNG\")\n",
    "buffer.seek(0)  # Reset the buffer's current position\n",
    "# Get the bytes\n",
    "img_bytes = buffer.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Add code snippet that shows the payload contents>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=model_name,\n",
    "    ContentType=\"image/*\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=img_bytes,\n",
    ")\n",
    "\n",
    "features = json.load(response[\"Body\"])[0]\n",
    "assert len(features) == 1536, f\"Unexpected features dimension.\"\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Delete the endpoint and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model_name)\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will perform batch inference using multiple input payloads together. If you are not familiar with batch transform, and want to learn more, see these links:\n",
    "1. [How it works](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-batch-transform.html)\n",
    "2. [How to run a batch transform job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"h-optimus-1\"\n",
    "content_type = \"application/x-image\"\n",
    "batch_transform_inference_instance_type = \"ml.g5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload your batch data to S3, note you can change the directory structure to something else depending on where you want to upload the files to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the batch-transform job input files to S3\n",
    "transform_input_folder = \"data/input/batch\"\n",
    "transform_input = sagemaker_session.upload_data(\n",
    "    transform_input_folder, key_prefix=model_name\n",
    ")\n",
    "print(\"Transform input uploaded to \" + transform_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory to store the output of the batch transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_output = f\"{transform_input}-output-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "print(transform_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model based on the parameters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "print(f\"Creating Model: {model_name}...\")\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,  # Replace with your IAM Role ARN\n",
    "    PrimaryContainer={\n",
    "        # This tells SageMaker to use the Model Package definition\n",
    "        \"ModelPackageName\": model_package_arn\n",
    "    },\n",
    "    EnableNetworkIsolation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the batch transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the transform job\n",
    "\n",
    "transform_job_name = f\"transform-job-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "print(f\"Starting Transform Job: {transform_job_name}...\")\n",
    "response = sm_client.create_transform_job(\n",
    "    TransformJobName=transform_job_name,\n",
    "    ModelName=model_name,  # Reference the model created in Step 1\n",
    "    MaxConcurrentTransforms=1,\n",
    "    MaxPayloadInMB=6,\n",
    "    BatchStrategy=\"MultiRecord\",\n",
    "    TransformInput={\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",  # Processing all files under the prefix\n",
    "                \"S3Uri\": transform_input\n",
    "            }\n",
    "        },\n",
    "        \"ContentType\": content_type,  # Change to \"application/json\" or \"application/x-image\" if needed\n",
    "        \"SplitType\": \"None\",        # Use \"None\" if passing whole files (e.g. images)\n",
    "        \"CompressionType\": \"None\"\n",
    "    },\n",
    "    TransformOutput={\n",
    "        \"S3OutputPath\": transform_output,\n",
    "        \"AssembleWith\": \"Line\",\n",
    "        \"Accept\": \"application/json\"        # The format you expect the output to be in\n",
    "    },\n",
    "    TransformResources={\n",
    "        \"TransformAmiVersion\": \"al2-ami-sagemaker-batch-gpu-535\",\n",
    "        \"InstanceType\": batch_transform_inference_instance_type,\n",
    "        \"InstanceCount\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Transform Job ARN: {response['TransformJobArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for completion\n",
    "\n",
    "print(\"Waiting for job to complete...\")\n",
    "start_time = time.time()\n",
    "waiter = sm_client.get_waiter('transform_job_completed_or_stopped')\n",
    "waiter.wait(TransformJobName=transform_job_name)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate duration\n",
    "duration_seconds = end_time - start_time\n",
    "minutes = int(duration_seconds // 60)\n",
    "seconds = int(duration_seconds % 60)\n",
    "\n",
    "# Check final status\n",
    "status = sm_client.describe_transform_job(TransformJobName=transform_job_name)\n",
    "    \n",
    "print(f\"   Job finished with status: {status['TransformJobStatus']}\")\n",
    "print(f\"   Total Wait Time: {minutes}m {seconds}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output is available on following path\n",
    "print(transform_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up model\n",
    "try:\n",
    "    sm_client.delete_model(ModelName=model_name)\n",
    "    print(f\"   Successfully deleted model: {model_name}\")\n",
    "except Exception as cleanup_error:\n",
    "    print(f\"   Warning: Could not delete model. It may have already been deleted or never created.\")\n",
    "    print(f\"   Error details: {cleanup_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the model package, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
